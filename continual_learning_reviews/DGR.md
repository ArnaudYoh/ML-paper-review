<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script>
[Continual Learning with Deep Generative Replay](http://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf)
======================================================================================================================================

Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon, 2017, NeurIPS
------------

Main Idea
---------

Model to avoid catastrophic forgetting.  Mentions episodic memory system and criticizes the large amount of memory that is needed.  Instead of  storing past experiences, why not replay them by generating them ?

Details
------

The model is made of Scholar tuples consisting of generator and a solver. The Generator reproduces real-like samples and the Solver is a task solving model. 

We have $N$ Scholars for $N$ tasks.  First, the Generator part is trained with current task data and those generated by older scholars. The solver is then trained on the real and data from previous scholars to pair inputs and targets. The Generator is based on WGAN-GP.

Final Thoughts
-------------

One of the paper introducing the idea of generative replay. Simple, yet efficient.